{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-06T08:53:43.853102Z"
    }
   },
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import (\n",
    "    VGG16, ResNet50, MobileNetV2, InceptionV3, EfficientNetB0\n",
    ")\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Setup and Configuration\n",
    "# ==============================================================================\n",
    "# Define the paths to your dataset directories.\n",
    "# IMPORTANT: Update these paths to match your local file structure.\n",
    "train_dir = r\"C:\\Users\\DELL\\PycharmProjects\\multi_fish_classifier\\train\"\n",
    "val_dir = r\"C:\\Users\\DELL\\PycharmProjects\\multi_fish_classifier\\val\"\n",
    "test_dir = r\"C:\\Users\\DELL\\PycharmProjects\\multi_fish_classifier\\test\"\n",
    "\n",
    "# Define model parameters\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "# The number of epochs for each training stage\n",
    "INITIAL_EPOCHS = 5\n",
    "FINE_TUNE_EPOCHS = 5\n",
    "TOTAL_EPOCHS = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n",
    "\n",
    "# The number of layers to unfreeze for fine-tuning.\n",
    "# A higher number means more layers of the base model are trained.\n",
    "FINE_TUNE_LAYERS = 25\n",
    "\n",
    "# Verify that the directories exist\n",
    "if not os.path.exists(train_dir) or not os.path.exists(val_dir) or not os.path.exists(test_dir):\n",
    "    raise FileNotFoundError(\n",
    "        \"One or more of the dataset directories were not found. \"\n",
    "        \"Please update the 'train_dir', 'val_dir', and 'test_dir' variables \"\n",
    "        \"at the top of the script with the correct paths.\"\n",
    "    )\n",
    "\n",
    "# Create a directory to save the plots\n",
    "plots_dir = Path(\"plots\")\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Data Preprocessing and Augmentation\n",
    "# ==============================================================================\n",
    "# Use ImageDataGenerator to load and preprocess images.\n",
    "# All images will be rescaled by 1/255.\n",
    "# Data augmentation is applied ONLY to the training data to prevent overfitting.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# The validation and test data should NOT be augmented, only rescaled.\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create data generators from the directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Keep data in order for evaluation\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Keep data in order for evaluation\n",
    ")\n",
    "\n",
    "# Dictionaries to hold the training history, trained models, and evaluation metrics\n",
    "histories = {}\n",
    "trained_models = {}\n",
    "evaluation_metrics = {}\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Transfer Learning with Fine-Tuning Function\n",
    "# ==============================================================================\n",
    "def build_and_train_model(base_model_name, base_model_class):\n",
    "    \"\"\"\n",
    "    Builds a transfer learning model, trains it in two stages (head training\n",
    "    and fine-tuning), and saves the best model during training.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Building and Training {base_model_name} Model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 3.1 Load the pre-trained base model\n",
    "    base_model = base_model_class(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=IMAGE_SIZE + (3,)\n",
    "    )\n",
    "\n",
    "    # 3.2 Add a new classification head on top of the base model\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # 3.3 Stage 1: Train the new classification head only\n",
    "    # Freeze all layers in the base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\n--- Starting Stage 1: Training the top layers ---\")\n",
    "    history_initial = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "        epochs=INITIAL_EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=val_generator.samples // BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # 3.4 Stage 2: Fine-tuning\n",
    "    # Unfreeze a portion of the base model's layers for fine-tuning\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-FINE_TUNE_LAYERS]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.00001), # Use a very low learning rate\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\n--- Starting Stage 2: Fine-tuning the base model ---\")\n",
    "    # Define a ModelCheckpoint callback to save the best model\n",
    "    checkpoint_filepath = f'best_model_{base_model_name}.h5'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=False, # Save the entire model\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    history_fine_tune = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "        epochs=TOTAL_EPOCHS,\n",
    "        initial_epoch=history_initial.epoch[-1],\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=val_generator.samples // BATCH_SIZE,\n",
    "        callbacks=[model_checkpoint_callback]\n",
    "    )\n",
    "\n",
    "    # Combine the histories for plotting\n",
    "    history_combined = {\n",
    "        'accuracy': history_initial.history['accuracy'] + history_fine_tune.history['accuracy'],\n",
    "        'val_accuracy': history_initial.history['val_accuracy'] + history_fine_tune.history['val_accuracy'],\n",
    "        'loss': history_initial.history['loss'] + history_fine_tune.history['loss'],\n",
    "        'val_loss': history_initial.history['val_loss'] + history_fine_tune.history['val_loss']\n",
    "    }\n",
    "\n",
    "    return model, history_combined\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Plotting Functions\n",
    "# ==============================================================================\n",
    "def plot_training_curves(history, model_name):\n",
    "    \"\"\"\n",
    "    Plots the training and validation accuracy and loss curves and saves them.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / f'{model_name.lower()}_training_curve.png')\n",
    "    plt.close() # Close the plot to free up memory\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred_classes, target_names, model_name):\n",
    "    \"\"\"\n",
    "    Computes and plots the confusion matrix, then saves it as a PNG file.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / f'{model_name.lower()}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_model_comparison(metrics):\n",
    "    \"\"\"\n",
    "    Creates and saves a bar chart comparing the final test accuracies of all models.\n",
    "    \"\"\"\n",
    "    model_names = list(metrics.keys())\n",
    "    accuracies = [metrics[name]['accuracy'] for name in model_names]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=model_names, y=accuracies, palette='viridis')\n",
    "    plt.title('Model Comparison: Test Accuracy')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.ylim(0, 1.0)\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir / 'model_comparison_table.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Main Execution Loop\n",
    "# ==============================================================================\n",
    "models_to_train = {\n",
    "    \"VGG16\": VGG16,\n",
    "    \"ResNet50\": ResNet50,\n",
    "    \"MobileNetV2\": MobileNetV2,\n",
    "    \"InceptionV3\": InceptionV3,\n",
    "    \"EfficientNetB0\": EfficientNetB0\n",
    "}\n",
    "\n",
    "# Loop through each model and train it\n",
    "for name, model_class in models_to_train.items():\n",
    "    model, history = build_and_train_model(name, model_class)\n",
    "    trained_models[name] = model\n",
    "    histories[name] = history\n",
    "\n",
    "    # After training, plot and save the training curves\n",
    "    plot_training_curves(history, name)\n",
    "\n",
    "    print(f\"\\nTraining curves for {name} saved to plots/{name.lower()}_training_curve.png\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Model Evaluation and Comparison\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting Final Evaluation on Test Set\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Run the evaluation for each trained model\n",
    "for name, model in trained_models.items():\n",
    "    test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)\n",
    "\n",
    "    print(f\"\\nFinal Evaluation for {name}:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Get predictions and true labels for detailed report and confusion matrix\n",
    "    y_pred = model.predict(test_generator)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = test_generator.classes\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(y_true, y_pred_classes, target_names=list(test_generator.class_indices.keys()))\n",
    "    print(report)\n",
    "\n",
    "    # Plot and save confusion matrix\n",
    "    plot_confusion_matrix(y_true, y_pred_classes, list(test_generator.class_indices.keys()), name)\n",
    "    print(f\"Confusion matrix for {name} saved to plots/{name.lower()}_confusion_matrix.png\")\n",
    "\n",
    "    # Store evaluation metrics for later comparison\n",
    "    metrics_dict = classification_report(y_true, y_pred_classes, target_names=list(test_generator.class_indices.keys()), output_dict=True)\n",
    "    evaluation_metrics[name] = {\n",
    "        'accuracy': test_accuracy,\n",
    "        'precision': metrics_dict['macro avg']['precision'],\n",
    "        'recall': metrics_dict['macro avg']['recall'],\n",
    "        'f1-score': metrics_dict['macro avg']['f1-score']\n",
    "    }\n",
    "\n",
    "# Finally, plot the comparison table\n",
    "plot_model_comparison(evaluation_metrics)\n",
    "print(\"\\nModel comparison bar chart saved to plots/model_comparison_table.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c3b2ee8d611ea85e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3cd87e4c18dffb8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
